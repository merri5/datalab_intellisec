{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import pickle \n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ps = PorterStemmer() \n",
    "spell = Speller()\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = os.path.join(os.getcwd(), \"ch02-train\")\n",
    "filenames_train = os.listdir(path_train)\n",
    "\n",
    "data = OrderedDict()\n",
    "labels = []\n",
    "for filename in filenames_train:\n",
    "    with open(os.path.join(os.getcwd(), \"ch02-train\", filename), mode = 'r', encoding = 'latin-1') as f:\n",
    "        try:\n",
    "            data[filename] = f.read()\n",
    "            labels.append(filename[-1])\n",
    "        except UnicodeDecodeError as error:\n",
    "            print(filename, \"   \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = os.path.join(os.getcwd(), \"ch02-test\")\n",
    "filenames_test = os.listdir(path_test)\n",
    "\n",
    "test = OrderedDict()\n",
    "for filename in filenames_test:\n",
    "    with open(os.path.join(os.getcwd(), \"ch02-test\", filename), mode = 'r', encoding = 'latin-1') as f:\n",
    "        try:\n",
    "            test[filename] = f.read()\n",
    "        except UnicodeDecodeError as error:\n",
    "            print(filename, \"   \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ise(text):\n",
    "    text = nltk.re.sub('[^A-Za-z]', ' ', text).lower()\n",
    "    tokens = nltk.re.sub(' +', ' ', text).strip().split(\" \")\n",
    "#     tokens = [ps.stem(spell(word)) for word in tokens]  # with spellcheck the function takes forever\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    \n",
    "    clear_tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    return \" \".join(clear_tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_data = Parallel(n_jobs = -1)(delayed(token_ise)(example) for example in data.values())\n",
    "tokenized_test = Parallel(n_jobs = -1)(delayed(token_ise)(example) for example in test.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(tokenized_data, labels, test_size = 0.1, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_opt = CountVectorizer(stop_words = stop)\n",
    "\n",
    "X_train_transformed = vectorizer_opt.fit_transform(X_train_1)\n",
    "X_test_transformed = vectorizer_opt.transform(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "knn.fit(X_train_transformed, y_train_1)\n",
    "\n",
    "preds = knn.predict(X_test_transformed)\n",
    "balanced_accuracy_score(preds, y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_test = CountVectorizer(stop_words = stop)\n",
    "\n",
    "X_train = vectorizer_test.fit_transform(tokenized_data)\n",
    "X_test = vectorizer_test.transform(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "knn.fit(X_train, labels)\n",
    "\n",
    "predictions_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions_test, index = test.keys()).to_csv(\"pred_of_test_data_KNN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
